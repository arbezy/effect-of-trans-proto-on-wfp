{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn Practise!\n",
    "CODE OBTAINED FROM wisepythagoras @ https://github.com/wisepythagoras/website-fingerprinting/blob/master/utils.py\n",
    "\n",
    "This code appears to use a 40 dimensional feature packets sizes (for the first 40 pkts), as well as 1 dimensional features: ratio of incoming to outgoing, number of incoming, number of outgoing, total number of pkts, total size of incoming packets.\n",
    "\n",
    "From this example, I think that the way you combine these multidimensional features with the single dimensional features is just by unpacking the multi and including the whole thing in one big array, then our training data should be an array of these combined feature arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump\n",
    "import random\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "def shuffle(x, y):\n",
    "    \"\"\" Shuffle the datasets. \"\"\"\n",
    "\n",
    "    for n in range(len(x) - 1):\n",
    "        rnd = random.randint(0, (len(x) - 1))\n",
    "        x1 = x[rnd]\n",
    "        x2 = x[rnd - 1]\n",
    "\n",
    "        y1 = y[rnd]\n",
    "        y2 = y[rnd - 1]\n",
    "\n",
    "        x[rnd - 1] = x1\n",
    "        x[rnd] = x2\n",
    "\n",
    "        y[rnd - 1] = y1\n",
    "        y[rnd] = y2\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def train(streams, labels):\n",
    "    \"\"\" This function trains the classifier with the data. \"\"\"\n",
    "\n",
    "    # Shuffle the arrays.\n",
    "    streams, labels = shuffle(streams, labels)\n",
    "\n",
    "    # TODO: I should do 10-fold cross validation like they do in the paper, this is like 90/10 but 10 times\n",
    "    # there are sklearn methods to do this\n",
    "    # NOTE: I think these streams / training data ratios may be wrong for me\n",
    "    \n",
    "    stream_amount = len(streams)\n",
    "    training_size = int(stream_amount * 0.9)\n",
    "\n",
    "    # Get 70% of the streams for training purposes.\n",
    "    training_x = streams[:training_size]\n",
    "    training_y = labels[:training_size]\n",
    "\n",
    "    # Get 30% of the streams for testing purposes\n",
    "    testing_x = streams[training_size:]\n",
    "    testing_y = labels[training_size:]\n",
    "\n",
    "    print(\"Training size: {}\".format(training_size))\n",
    "    print(\"Testing size:  {}\".format(stream_amount - training_size))\n",
    "\n",
    "    # Initialize the classifier.\n",
    "    # NOTE: I will not use KNeighbours, instead random forest\n",
    "    clf = KNeighborsClassifier()\n",
    "\n",
    "    # Now lets train our KNN classifier.\n",
    "    clf = clf.fit(training_x, training_y)\n",
    "\n",
    "    # Save a snapshot of this classifier.\n",
    "    dump(clf, \"./classifier-nb.dmp\", compress=9)\n",
    "\n",
    "    # Get the prediction.\n",
    "    predictions = clf.predict(testing_x)\n",
    "\n",
    "    print(\"Accuracy: %s%%\" % (accuracy_score(testing_y, predictions) * 100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME CODE I FOUND THE WEBBBB\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import utils\n",
    "\n",
    "\n",
    "# Read the configuration and start training.\n",
    "# NOTE: config stores a list of the websites that have been captured (more specifically the domain names)\n",
    "with open('config.json') as fp:\n",
    "    print(\"* Parsing configuration\")\n",
    "\n",
    "    # Load the configuration from the file.\n",
    "    config = json.load(fp)\n",
    "\n",
    "    # This is where all the streams are going to live.\n",
    "    streams = []\n",
    "\n",
    "    # This is where all the labels are going to live.\n",
    "    labels = []\n",
    "    labels_str = []\n",
    "    base_labels = [None] * len(config['pcaps'])\n",
    "\n",
    "    # The base label starts from 1 and increments after that.\n",
    "    current_label = 1\n",
    "    # NOTE: this would not match my format, also re == regex\n",
    "    pat = re.compile(\".*-curl\\.pcap$\")\n",
    "\n",
    "    for domain in config['pcaps']:\n",
    "        # Set the base label.\n",
    "        base_labels[current_label - 1] = domain\n",
    "\n",
    "        # Increment the label\n",
    "        current_label += 1\n",
    "\n",
    "    #utils.empty_csv()\n",
    "    current_label = 1\n",
    "\n",
    "    for domain in config['pcaps']:\n",
    "        print(\" - {}\".format(domain))\n",
    "        i = 0\n",
    "\n",
    "        # Traverse the directory for all the pcaps.\n",
    "        # NOTE: my files don't match this format\n",
    "        for file in os.listdir('./pcaps/{}'.format(domain)):\n",
    "            if file.endswith(\".pcap\") and (pat.match(file) is None):\n",
    "                # if i > 20:\n",
    "                #     break\n",
    "\n",
    "                # This is the pcap file we'll be reading at this point.\n",
    "                file = os.path.join(\"./pcaps/{}\".format(domain), file)\n",
    "\n",
    "                # Read the pcap file.\n",
    "                data = utils.read_pcap_file(file)\n",
    "\n",
    "                # Append the data to the streams array.\n",
    "                streams.append(data)\n",
    "\n",
    "                # Append everything to the log.\n",
    "                utils.append_to_csv(domain, data)\n",
    "\n",
    "                # Add a label for the new file.\n",
    "                labels.append(current_label)\n",
    "                labels_str.append(domain)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "        print(f\"    {i} pcap files\")\n",
    "\n",
    "        # Increment the label\n",
    "        current_label += 1\n",
    "\n",
    "    # Finally train the classifier.\n",
    "    utils.train(streams, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR LOGGING\n",
    "\n",
    "def empty_log():\n",
    "    \"\"\" Empties the CSV file. \"\"\"\n",
    "\n",
    "    with open(\"./fingerprints.csv\", 'w') as f:\n",
    "        f.write(\"\")\n",
    "\n",
    "\n",
    "def append_to_log(domain, data):\n",
    "    \"\"\" Append the information to the log file. \"\"\"\n",
    "\n",
    "    with open(\"./fingerprints.csv\", 'a') as f:\n",
    "        f.write(\"{},{}\\n\".format(domain, ','.join(str(num) for num in data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR CONTEXT THIS IS WHAT THEIR READ PCAP FUNC LOOKS LIKE:\n",
    "\n",
    "def read_pcap_file(file):\n",
    "    \"\"\" Read the pcap file and return the sizes of the packets. \"\"\"\n",
    "\n",
    "    # Read the file.\n",
    "    fp = open(file, 'rb')\n",
    "\n",
    "    # Create the pcap object\n",
    "    pcap = dpkt.pcap.Reader(fp)\n",
    "\n",
    "    # This is the array that will contain all the packet sizes.\n",
    "    sizes = [0] * 40\n",
    "    i = 0\n",
    "\n",
    "    # Hold the addresses of the outgoing agent.\n",
    "    outgoing_addr = None\n",
    "\n",
    "    outgoing_packets = 0\n",
    "    incoming_packets = 0\n",
    "    total_number_of_packets = 0\n",
    "\n",
    "    # This will contain the total size of the incoming packets.\n",
    "    incoming_size = 0\n",
    "\n",
    "    # Loop through all the packets and save the sizes.\n",
    "    for ts, buf in pcap:\n",
    "        packet_size = len(buf)\n",
    "        is_outgoing = True\n",
    "\n",
    "        # Parse the Ethernet packet.\n",
    "        eth = dpkt.ethernet.Ethernet(buf)\n",
    "\n",
    "        # Parse the IP packet.\n",
    "        ip = eth.data\n",
    "\n",
    "        # Get the source addresses.\n",
    "        src = inet_to_str(ip.src)\n",
    "\n",
    "        if total_number_of_packets == 0:\n",
    "            # Get the address of the outgoing agents. The target user is the\n",
    "            # outgoing agent, and the incoming packets are the server/website.\n",
    "            outgoing_addr = src\n",
    "            outgoing_packets += 1\n",
    "\n",
    "        elif src == outgoing_addr:\n",
    "            # Increment the outgoing packets.\n",
    "            outgoing_packets += 1\n",
    "\n",
    "        else:\n",
    "            # Increment the incoming packets.\n",
    "            incoming_packets += 1\n",
    "\n",
    "            # Increment the size of the incoming packets.\n",
    "            incoming_size += packet_size\n",
    "\n",
    "            # This is an incoming packet.\n",
    "            is_outgoing = False\n",
    "\n",
    "        if i < 40:\n",
    "            # Add the size to the array.\n",
    "            sizes[i] = packet_size if is_outgoing else -packet_size\n",
    "\n",
    "            # Increment the index.\n",
    "            i += 1\n",
    "\n",
    "        # Increment the total amount of packets.\n",
    "        total_number_of_packets += 1\n",
    "\n",
    "    # Get the ratio.\n",
    "    ratio = float(incoming_packets) / (outgoing_packets if outgoing_packets != 0 else 1)\n",
    "\n",
    "    # Print some details.\n",
    "    print(f'OUT: {outgoing_packets},' +\n",
    "            f'IN: {incoming_packets},' +\n",
    "            f'TOTAL: {total_number_of_packets},' +\n",
    "            f'SIZE: {incoming_size},' +\n",
    "            f'RATIO: {ratio}')\n",
    "\n",
    "    # Reverse the array to append the other information.\n",
    "    sizes.reverse()\n",
    "\n",
    "    # Add the ratio of incoming to outgoing packets.\n",
    "    sizes.append(ratio)\n",
    "\n",
    "    # Add the number of incoming packets.\n",
    "    sizes.append(incoming_packets)\n",
    "\n",
    "    # Add the number of outgoing packets.\n",
    "    sizes.append(outgoing_packets)\n",
    "\n",
    "    # Add the number of total packets.\n",
    "    sizes.append(total_number_of_packets)\n",
    "\n",
    "    # Add the total size of the incoming packets.\n",
    "    sizes.append(incoming_size)\n",
    "\n",
    "    # Reverse the array again so that the sizes are in order.\n",
    "    sizes.reverse()\n",
    "\n",
    "    # Finally return the sizes.\n",
    "    return sizes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
