{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCAP tests + graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = sys.argv[1]\n",
    "path = 'D:/FINALQUICRESULTS/h3results'\n",
    "\n",
    "\n",
    "pcap_file_sizes_grouped = {fn:[] for fn in list(map(lambda x: x.split('_')[1], os.listdir(path))) if fn.endswith('.pcap')}\n",
    "pcaps = {fn:[] for fn in list(map(lambda x: x.split('_')[1], os.listdir(path))) if fn.endswith('.pcap')}\n",
    "\n",
    "\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    filepath = os.path.join(path, f)\n",
    "    if f.endswith('.pcap'):\n",
    "        #pcap_filenames.append(f)\n",
    "        # returns file size in bytes\n",
    "        pcap_size = os.path.getsize(filepath)\n",
    "        pcap_file_sizes_grouped[f.split('_')[1]].append(pcap_size)\n",
    "        pcaps[f.split('_')[1]].append((f, pcap_size))\n",
    "        \n",
    "print(pcap_file_sizes_grouped)\n",
    "print(pcap_file_sizes_grouped.keys())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "h2domains = [fn for fn in list(map(lambda x: x.split('_')[1], os.listdir(\"D:/FINALH2RESULTS/h2results\"))) if fn.endswith('.pcap')]\n",
    "print(len(set(h2domains)))\n",
    "h3domains = [fn for fn in list(map(lambda x: x.split('_')[1], os.listdir(\"D:/FINALQUICRESULTS/h3results\"))) if fn.endswith('.pcap')]\n",
    "print(len(set(h3domains)))\n",
    "print(set(h3domains).symmetric_difference(set(h2domains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# boxplots for all pcaps\n",
    "# sorting by mean so it looks nicer\n",
    "yss = list(pcap_file_sizes_grouped.values())\n",
    "\n",
    "means = [sum(yss[i]) / len(yss[i]) for i in range(len(pcap_file_sizes_grouped))]\n",
    "yss_zip = sorted(list(zip(means, yss)))\n",
    "xss_zip = sorted(list(zip(means, list(pcap_file_sizes_grouped.keys()))))\n",
    "yss_sorted_by_mean = list(map(lambda x: x[1], yss_zip))\n",
    "xss_sorted_by_mean = list(map(lambda x: x[1], xss_zip))\n",
    "xss_sorted_by_mean = list(map(lambda x: x[:-5], xss_sorted_by_mean))\n",
    "print(yss_sorted_by_mean)\n",
    "\n",
    "\n",
    "# printing outlier file names:\n",
    "print(pcaps)\n",
    "outliers={site:[] for site in pcaps.keys()}\n",
    "no_of_outliers={site:0 for site in pcaps.keys()}\n",
    "for k in pcaps.keys():\n",
    "    ps = pcaps[k]\n",
    "    q1 = np.percentile(list(map(lambda x: x[1], pcaps[k])), 25, method='midpoint')\n",
    "    q3 = np.percentile(list(map(lambda x: x[1], pcaps[k])), 75, method='midpoint')\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + (1.5 * iqr)\n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "    for (fname, fsize) in ps:\n",
    "        if fsize > upper_bound:\n",
    "            print(f\"{fname} is too large\")\n",
    "            outliers[k].append(fname)\n",
    "            no_of_outliers[k] += 1\n",
    "        elif fsize < lower_bound:\n",
    "            print(f\"{fname} is too small\")\n",
    "            outliers[k].append(fname)\n",
    "            no_of_outliers[k] += 1\n",
    "        \n",
    "print()\n",
    "for k in no_of_outliers:\n",
    "    t=no_of_outliers[k]\n",
    "    no_of_site_captures = len(pcaps[k])\n",
    "    if t > 0 and t <= 10:\n",
    "        print(f\"{k} has {t} outliers\")\n",
    "        if no_of_site_captures - t >= 100:\n",
    "            for o in outliers[k]:\n",
    "                del_path=path + '/' + o\n",
    "                #os.remove(del_path)\n",
    "                print(\"outlier deleted\")\n",
    "        else:\n",
    "            print(\"cannot safely delete outliers\")\n",
    "    elif t > 10:\n",
    "        print(f\"{k} has {t} outliers\")\n",
    "        \n",
    "        print(f\"{no_of_site_captures} - {t} = {no_of_site_captures-t} (if greater than 100 then safe to delete)\")\n",
    "        if no_of_site_captures-t >= 100:\n",
    "            for o in outliers[k]:\n",
    "                del_path=path + '/' + o\n",
    "                #os.remove(del_path)\n",
    "            print(\"outliers deleted\")\n",
    "        else:\n",
    "            print(\"NOT A SAFE DELETE\")\n",
    "            \n",
    "for k in pcaps:\n",
    "    no_of_site_captures = len(pcaps[k])\n",
    "    if no_of_site_captures < 100:\n",
    "            print(f\"ERROR: less than 100 captures for site {k}, pcaps left = {no_of_site_captures}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(yss_sorted_by_mean[0:20], showfliers=False)\n",
    "plt.ylim([0, 1500000])\n",
    "plt.xticks(range(1, 21), xss_sorted_by_mean[0:20], rotation=90)\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Packet Capture Size (Bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.boxplot(yss_sorted_by_mean[20:40], showfliers=False)\n",
    "plt.ylim(0, 2500000)\n",
    "plt.xticks(range(1, 21), xss_sorted_by_mean[20:40], rotation=90)\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Packet Capture Size (Bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(yss_sorted_by_mean[40:60], showfliers=False)\n",
    "plt.xticks(range(1, 21), xss_sorted_by_mean[40:60], rotation=90)\n",
    "plt.ylim(1000000, 3500000)\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Packet Capture Size (Bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(yss_sorted_by_mean[60:80])\n",
    "plt.xticks(range(1, 21), xss_sorted_by_mean[60:80], rotation=90)\n",
    "plt.ylim(2500000, 7000000)\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Packet Capture Size (Bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(yss_sorted_by_mean[80:])\n",
    "plt.xticks(range(1, len(xss_sorted_by_mean[80:])+1), xss_sorted_by_mean[80:], rotation=90)\n",
    "plt.ylim(5000000, 45000000)\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Packet Capture Size (Bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of all data...\n",
    "import seaborn as sns\n",
    "\n",
    "listed_values = list(pcap_file_sizes_grouped.values())\n",
    "flat_sizes = [v for i in range(len(listed_values)) for v in listed_values[i]]\n",
    "print(flat_sizes)\n",
    "sns.histplot(flat_sizes)\n",
    "\n",
    "plt.xlabel(\"Packet Capture Size (Bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pcap_file_sizes_grouped))\n",
    "for k in pcap_file_sizes_grouped:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for empty files\n",
    "# NOTE: The pcap files will rarely be empty if broad filter was used when collecting traffic.\n",
    "#       Better to filter to the relevant packets only then check if they are empty... \n",
    "#       But this can only be done on parsed traffic...\n",
    "\n",
    "for k in pcap_file_sizes_grouped.keys():\n",
    "    for f_size in pcap_file_sizes_grouped[k]:\n",
    "        if f_size == 0:\n",
    "            print(f\"ERROR: one of the {k} pcaps is empty\")\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN AND STANDARD DEVIATION FOR ALL CRAWLS TO A WEBSITE\n",
    "\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# list_of_sites = ['ac', 'anu', 'berkeley', 'bristol', 'brown', 'bu', 'caltech',\n",
    "#        'cam', 'ch', 'charite', 'cmu', 'columbia', 'cornell', 'cuhk',\n",
    "#        'dartmouth', 'de', 'ed', 'edu', 'emory', 'epfl', 'eu', 'eur',\n",
    "#        'gatech', 'gla', 'harvard', 'helsinki', 'hku', 'hu-berlin',\n",
    "#        'imperial', 'jhu', 'kcl', 'kuleuven', 'kyoto-u', 'lmu', 'lse',\n",
    "#        'lunduniversity', 'manchester', 'mcgill', 'mcmaster', 'mit',\n",
    "#        'monash', 'northwestern', 'ntu', 'nyu', 'osu', 'ox', 'pku',\n",
    "#        'princeton', 'psu', 'purdue', 'rice', 'rug', 'rwth-aachen', 'se',\n",
    "#        'skku', 'snu', 'sorbonne-universite', 'stanford', 'sydney',\n",
    "#        'tsinghua', 'tudelft', 'tum', 'u-tokyo', 'ubc', 'ucdavis',\n",
    "#        'uchicago', 'ucl', 'ucla', 'ucsb', 'umn', 'umontreal', 'unc',\n",
    "#        'uni-heidelberg', 'unimelb', 'universiteitleiden', 'unsw', 'upenn',\n",
    "#        'uq', 'usc', 'ustc', 'utexas', 'utoronto', 'uu', 'uva', 'uzh',\n",
    "#        'washington', 'wisc', 'wur', 'yale']\n",
    "\n",
    "# pcap_names = os.listdir(path)\n",
    "\n",
    "pcap_filenames = []\n",
    "pcap_filesizes = []\n",
    "\n",
    "filesize_zipped = list(zip(pcap_filenames, pcap_filesizes))\n",
    "\n",
    "sorting_filesize_zipped = sorted(list(map(lambda x: (x[0][2:], x[1]), filesize_zipped)))\n",
    "\n",
    "# grouping the files by website\n",
    "just_the_sizes_grouped = []\n",
    "builder = []\n",
    "temp = sorting_filesize_zipped[0][0]\n",
    "for i in range(len(sorting_filesize_zipped)):\n",
    "    (name, size) = sorting_filesize_zipped[i]\n",
    "    if name != temp:\n",
    "        temp = name\n",
    "        just_the_sizes_grouped.append((temp, builder))\n",
    "        builder = []\n",
    "    builder.append(size)\n",
    "    \n",
    "    # if its the last element then push the file sizes anyway\n",
    "    if sorting_filesize_zipped[-1] == sorting_filesize_zipped[i]:\n",
    "        just_the_sizes_grouped.append((temp, builder))\n",
    "    \n",
    "print(just_the_sizes_grouped)\n",
    "\n",
    "# calculate mean and std deviation of all crawls to a website\n",
    "means = {}\n",
    "std_devs = {}\n",
    "for (name, size_list) in just_the_sizes_grouped:\n",
    "    means[name] = mean(size_list)\n",
    "    std_devs[name] = stdev(size_list)\n",
    "    # outlier threshold = mean +/- 3 std devs\n",
    "    lower_threshold = means[name] - 3*std_devs[name]\n",
    "    upper_threshold = means[name] + 3*std_devs[name]\n",
    "    for s in size_list:\n",
    "        if s < lower_threshold or s > upper_threshold:\n",
    "            print(f\"outlier found in {name}: value = {s}\")\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "means = {}\n",
    "std_devs = {}\n",
    "for name in pcap_file_sizes_grouped:\n",
    "    size_list = pcap_file_sizes_grouped[name]\n",
    "    means[name] = mean(size_list)\n",
    "    std_devs[name] = stdev(size_list)\n",
    "    # outlier threshold = mean +/- 3 std devs\n",
    "    lower_threshold = means[name] - 3*std_devs[name]\n",
    "    upper_threshold = means[name] + 3*std_devs[name]\n",
    "    for s in size_list:\n",
    "        if s < lower_threshold:\n",
    "            print(f\"outlier found in {name}: value = {s} (big)\")\n",
    "        elif s > upper_threshold:\n",
    "            print(f\"outlier found in {name}: value = {s} (small)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check screenshots\n",
    "\n",
    "from PIL import ImageChops\n",
    "import math, operator\n",
    "\n",
    "def rmsdiff(im1, im2):\n",
    "    \"Calculate the root-mean-square difference between two images\"\n",
    "\n",
    "    h = ImageChops.difference(im1, im2).histogram()\n",
    "\n",
    "    # calculate rms\n",
    "    return math.sqrt(reduce(operator.add,\n",
    "        map(lambda h, i: h*(i**2), h, range(256))\n",
    "    ) / (float(im1.size[0]) * im1.size[1]))\n",
    "    \n",
    "    \n",
    "# deprecated, just checking by eye instead.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsed pcap tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the smallest and largest packets\n",
    "# find the number of packets\n",
    "# find the mean packet size\n",
    "# print the ratio of incoming to outgoing \n",
    "# check there are actually incoming and outgoing packets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# could store these stats so they are easier to compare to other captures of the same website\n",
    "\n",
    "# could label df to make a little easier to read\n",
    "cols = {'capture_name':[], 'smallest_pkt':[], 'largest_pkt':[], 'mean_pkt':[], 'ratio':[], 'negative':[]}\n",
    "pkt_tests = pd.DataFrame(cols)\n",
    "\n",
    "root_path = 'C:/Users/andre/Uni_Documents/Dissertation/website-fingerprinting/parsed_results/h3results/parsed-h3-results'\n",
    "\n",
    "for f in os.listdir(root_path):\n",
    "    largest_pkt_size = 0\n",
    "    smallest_pkt_size = 99999\n",
    "    fpath = os.path.join(root_path, f)\n",
    "    current_capture = pd.read_csv(fpath, sep='\\t')\n",
    "    current_cap_pkt_sizes = []\n",
    "    positive = 1\n",
    "    negative = 1\n",
    "    print(\"current file:\", f)\n",
    "    for index, row in current_capture.iterrows():\n",
    "        if row[3] == 2020 or row[4] == 2020 or row[5] == 2020 or row[6] == 2020:\n",
    "            pkt_size = row[10]\n",
    "            current_cap_pkt_sizes.append(pkt_size)\n",
    "            if pkt_size < smallest_pkt_size:\n",
    "                smallest_pkt_size = pkt_size\n",
    "            if pkt_size > largest_pkt_size:\n",
    "                largest_pkt_size = pkt_size\n",
    "                \n",
    "            if row[4] == 2020 or row[6] == 2020:\n",
    "                positive += 1\n",
    "            elif row[3] == 2020 or row[5] == 2020:\n",
    "                negative += 1\n",
    "    print(f\"capture {f}: smallest={smallest_pkt_size}, largest={largest_pkt_size}, mean size={np.mean(current_cap_pkt_sizes)}, ratio={positive / negative}, -ve={negative-1}\")\n",
    "    pkt_tests.loc[-1] = [f, smallest_pkt_size, largest_pkt_size, np.mean(current_cap_pkt_sizes), positive/negative, negative-1]\n",
    "    pkt_tests.index = pkt_tests.index + 1\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus_caps = pkt_tests[pkt_tests['negative'] == 0]\n",
    "sus_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for all collected pkts:\n",
    "print(f\"smallest pkt = {pkt_tests['smallest_pkt'].min()}\")\n",
    "print(f\"largest pkt = {pkt_tests['largest_pkt'].max()}\")\n",
    "print(f\"mean pkt = {pkt_tests['mean_pkt'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the smallest and largest packets\n",
    "# find the number of packets\n",
    "# find the mean packet size\n",
    "# print the ratio of incoming to outgoing \n",
    "# check there are actually incoming and outgoing packets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# could store these stats so they are easier to compare to other captures of the same website\n",
    "\n",
    "# could label df to make a little easier to read\n",
    "cols = {'capture_name':[], 'smallest_pkt':[], 'largest_pkt':[], 'mean_pkt':[], 'ratio':[], 'negative':[]}\n",
    "pkt_tests = pd.DataFrame(cols)\n",
    "\n",
    "root_path = 'D:/FINALH2RESULTS/parsed-h2-results'\n",
    "\n",
    "for f in os.listdir(root_path):\n",
    "    largest_pkt_size = 0\n",
    "    smallest_pkt_size = 99999\n",
    "    fpath = os.path.join(root_path, f)\n",
    "    current_capture = pd.read_csv(fpath, sep='\\t')\n",
    "    current_cap_pkt_sizes = []\n",
    "    positive = 1\n",
    "    negative = 1\n",
    "    print(\"current file:\", f)\n",
    "    for index, row in current_capture.iterrows():\n",
    "        if row[3] == 2020 or row[4] == 2020 or row[5] == 2020 or row[6] == 2020:\n",
    "            pkt_size = row[12]\n",
    "            current_cap_pkt_sizes.append(pkt_size)\n",
    "            if pkt_size < smallest_pkt_size:\n",
    "                smallest_pkt_size = pkt_size\n",
    "            if pkt_size > largest_pkt_size:\n",
    "                largest_pkt_size = pkt_size\n",
    "                \n",
    "            if row[4] == 2020 or row[6] == 2020:\n",
    "                positive += 1\n",
    "            elif row[3] == 2020 or row[5] == 2020:\n",
    "                negative += 1\n",
    "    print(f\"capture {f}: smallest={smallest_pkt_size}, largest={largest_pkt_size}, mean size={np.mean(current_cap_pkt_sizes)}, ratio={positive / negative}, -ve={negative-1}\")\n",
    "    pkt_tests.loc[-1] = [f, smallest_pkt_size, largest_pkt_size, np.mean(current_cap_pkt_sizes), positive/negative, negative-1]\n",
    "    pkt_tests.index = pkt_tests.index + 1\n",
    "        \n",
    "        \n",
    "    \n",
    "print(f\"smallest pkt = {pkt_tests['smallest_pkt'].min()}\")\n",
    "print(f\"largest pkt = {pkt_tests['largest_pkt'].max()}\")\n",
    "print(f\"mean pkt = {pkt_tests['mean_pkt'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkt_tests.sort_values(by=['negative']).tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any missed sites? yes usually...\n",
    "import os\n",
    "import sys\n",
    "target_sites = []\n",
    "crawled_sites = []\n",
    "for f in os.listdir(\"C:/Users/andre/Uni_Documents/Dissertation/website-fingerprinting/parsed_results/h3results/parsed-h3-results\"):\n",
    "    crawled_sites.append(f.split('_')[1].split('.')[0])\n",
    "crawled_sites = list(set(crawled_sites))\n",
    "print(f\"crawled {len(crawled_sites)} sites\")\n",
    "\n",
    "for s in sorted(crawled_sites):\n",
    "    print(s)\n",
    "\n",
    "\n",
    "# SOME URLS W/O www. in their url have failed their crawl due to the way I name output pcaps, these will need to be recrawled...\n",
    "# A list of these is: (sites with a ~ were probs saved but not saved as the right name)\n",
    "# 1- ethz.ch\n",
    "# 2- umich.edu\n",
    "# 3- duke.edu\n",
    "# 4- nus.edu.sg\n",
    "# 5- ucsd.edu\n",
    "# 6- ki.se \n",
    "# 7- kust.edu.uk ~\n",
    "# 8- psl.eu ~\n",
    "# 9- illinois.edu\n",
    "# 10- wustl.edu\n",
    "# 11- en.snu.ac.kr ~ -> nah this one is good\n",
    "# 12- win-cities.umn.edu ~\n",
    "# 13- uni-freiburg.de\n",
    "# 14- warwick.ac.uk ~\n",
    "# 15- umd.edu \n",
    "# 16- uni-teubingen.de ~\n",
    "# 17- uci.edu\n",
    "# 18- msu.edu\n",
    "\n",
    "# need to delete: edu, eu, ac, de, ch, umn, and replace with these newly crawled sites\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
